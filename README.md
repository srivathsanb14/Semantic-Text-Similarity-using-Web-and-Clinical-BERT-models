# Semantic-Text-Similarity-using-Web-and-Clinical-BERT-models

In the notebook, I have explored Semantic Text Similarity using BERT models. 

The following process was followed:
1.	First the text had to be pre-processed. Preprocessing was carried out: stopwords were removed and shortened phrases were expanded. 
2.	A model had to built or imported. I imported two BERT similarity models to carry out the Semantic Text Similarity task. 
3.	The preprocessed data is passed through the built/imported models and the similarity score is calculated. 
4.	The similarity score is pushed onto a new data frame along with the corresponding Unique ID.
