{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGjzpq26rK5i"
   },
   "source": [
    "# SEMANTIC TEXT SIMILARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhtrZUm3rQa2"
   },
   "source": [
    "In this notebook, I explore how to perform Semantic Text Similarity using BERT models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggsps9OFrIGU"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vYHCDDIdLIY",
    "outputId": "514a9dd2-48e7-409e-b3bc-29e0af5d15ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm                    #For status bar display\n",
    "\n",
    "import collections\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer  # For lemmatization of words\n",
    "from nltk.corpus import stopwords        # Loading list of stopwords\n",
    "from nltk import word_tokenize           # Converting paragraph in tokens\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tKcpNXLf5kh"
   },
   "source": [
    "# Reading the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "VJdqkZRddTlo",
    "outputId": "1b22bf20-d3ba-4235-f232-bad017739358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of text_data :  (4023, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  ...                                              text2\n",
       "0          0  ...  newcastle 2-1 bolton kieron dyer smashed home ...\n",
       "1          1  ...  nasdaq planning $100m share sale the owner of ...\n",
       "2          2  ...  ruddock backs yapp s credentials wales coach m...\n",
       "3          3  ...  mci shares climb on takeover bid shares in us ...\n",
       "4          4  ...  media gadgets get moving pocket-sized devices ...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading given data-set using pandas\n",
    "text_data = pd.read_csv(\"Text_Similarity_Dataset.csv\")\n",
    "print(\"Shape of text_data : \", text_data.shape)\n",
    "text_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6LLabkugB8l"
   },
   "source": [
    "Checking if the dataset has any null values and dropping those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2U_dUesdy5l",
    "outputId": "f939e302-cdaa-4cc6-8867-2d442965b764"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique_ID    0\n",
       "text1        0\n",
       "text2        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.isnull().sum() # Check if text data have any null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "pz5oXCLhd08l",
    "outputId": "5fd60d07-c75a-4e4c-b921-76a2abafa93a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>4018</td>\n",
       "      <td>labour plans maternity pay rise maternity pay ...</td>\n",
       "      <td>no seasonal lift for house market a swathe of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4019</td>\n",
       "      <td>high fuel costs hit us airlines two of the lar...</td>\n",
       "      <td>new media battle for bafta awards the bbc lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>4020</td>\n",
       "      <td>britons growing  digitally obese  gadget lover...</td>\n",
       "      <td>film star fox behind theatre bid leading actor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>4021</td>\n",
       "      <td>holmes is hit by hamstring injury kelly holmes...</td>\n",
       "      <td>tsunami  to hit sri lanka banks  sri lanka s b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>4022</td>\n",
       "      <td>nuclear dumpsite  plan attacked plans to allow...</td>\n",
       "      <td>x factor show gets second series tv talent sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_ID  ...                                              text2\n",
       "0             0  ...  newcastle 2-1 bolton kieron dyer smashed home ...\n",
       "1             1  ...  nasdaq planning $100m share sale the owner of ...\n",
       "2             2  ...  ruddock backs yapp s credentials wales coach m...\n",
       "3             3  ...  mci shares climb on takeover bid shares in us ...\n",
       "4             4  ...  media gadgets get moving pocket-sized devices ...\n",
       "...         ...  ...                                                ...\n",
       "4018       4018  ...  no seasonal lift for house market a swathe of ...\n",
       "4019       4019  ...  new media battle for bafta awards the bbc lead...\n",
       "4020       4020  ...  film star fox behind theatre bid leading actor...\n",
       "4021       4021  ...  tsunami  to hit sri lanka banks  sri lanka s b...\n",
       "4022       4022  ...  x factor show gets second series tv talent sho...\n",
       "\n",
       "[4023 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXsuJXFwglAa"
   },
   "source": [
    "# Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LREAEy2Bgop9"
   },
   "source": [
    "Expanding shortened phrases, removing stopwords, removing special symbols and converting all characters into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "WrEkMFCUd7Jv"
   },
   "outputs": [],
   "source": [
    "def expansion(phrase):    #expanding shortened phrases and removing stopwords using a function\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)       #expanding 'wont' to 'will not'\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)       #expanding 'cant' to 'can not'\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)            #expanding suffix n't to not\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)            #expanding suffix 're to are \n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)              #expanding suffix 's to is \n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)           #expanding suffix 'd to would \n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)           #expanding suffix 'll to will\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)           #expanding suffix 've to have\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)              #expanding suffix 'm to am\n",
    "    #I might have missed some shortened phrases. If so, they can easily be added here\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_WvIxB_roBF"
   },
   "source": [
    "Prepocessing first column of dataset, i.e. \"text1\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKPkD85md_3-",
    "outputId": "929b8f82-b4b6-4e39-ae79-feea1d647fe8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4023/4023 [02:30<00:00, 26.82it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text1 = []    #Defining an alternate preprocessed text list for text1\n",
    "\n",
    "for sentence in tqdm(text_data['text1'].values):\n",
    "    sent = expansion(sentence)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text1.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "iNTDjc-neCh7",
    "outputId": "7d88e66b-84fd-460a-c8a2-80955cc0a75a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail spot ads internet search ...</td>\n",
       "      <td>newcastle 2 1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions miss net 2025 40 uk population still ...</td>\n",
       "      <td>nasdaq planning 100m share sale owner technolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short ginepri fifteen year old...</td>\n",
       "      <td>ruddock backs yapp credentials wales coach mik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo buy us wine firm diageo world biggest s...</td>\n",
       "      <td>mci shares climb takeover bid shares us phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>careful code new european directive could put ...</td>\n",
       "      <td>media gadgets get moving pocket sized devices ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  ...                                              text2\n",
       "0          0  ...  newcastle 2 1 bolton kieron dyer smashed home ...\n",
       "1          1  ...  nasdaq planning 100m share sale owner technolo...\n",
       "2          2  ...  ruddock backs yapp credentials wales coach mik...\n",
       "3          3  ...  mci shares climb takeover bid shares us phone ...\n",
       "4          4  ...  media gadgets get moving pocket sized devices ...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging preprocessed_text1 in place of text1 in text_data\n",
    "text_data['text1'] = preprocessed_text1\n",
    "text_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f8E6gOeruyj"
   },
   "source": [
    "Prepocessing second column of dataset, i.e. \"text2\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTqBbDc4enF3",
    "outputId": "0f33c897-8416-4d3e-cb06-0008ab531a21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4023/4023 [02:29<00:00, 26.85it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text2 = []    #Defining an alternate preprocessed text list for text2\n",
    "\n",
    "for sentence in tqdm(text_data['text2'].values):\n",
    "    sent = decontracted(sentence)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "   \n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text2.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0-Pzmp1Bep1g",
    "outputId": "deb544f7-5aa1-4bd7-b1e9-455e927649a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail spot ads internet search ...</td>\n",
       "      <td>newcastle 2 1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions miss net 2025 40 uk population still ...</td>\n",
       "      <td>nasdaq planning 100m share sale owner technolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short ginepri fifteen year old...</td>\n",
       "      <td>ruddock backs yapp credentials wales coach mik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo buy us wine firm diageo world biggest s...</td>\n",
       "      <td>mci shares climb takeover bid shares us phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>careful code new european directive could put ...</td>\n",
       "      <td>media gadgets get moving pocket sized devices ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  ...                                              text2\n",
       "0          0  ...  newcastle 2 1 bolton kieron dyer smashed home ...\n",
       "1          1  ...  nasdaq planning 100m share sale owner technolo...\n",
       "2          2  ...  ruddock backs yapp credentials wales coach mik...\n",
       "3          3  ...  mci shares climb takeover bid shares us phone ...\n",
       "4          4  ...  media gadgets get moving pocket sized devices ...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging preprocessed_text2 in place of text2 in text_data\n",
    "text_data['text2'] = preprocessed_text2\n",
    "text_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV9aNgkBpzWr"
   },
   "source": [
    "#Word tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqqm_cCIp3DL"
   },
   "source": [
    "We now build a function which tokenizes and lemmatizes the sentences into words. However this is not used in the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "VtyTxDijfXVo"
   },
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "            #tokenizes, stems and lemmatizes the text. Though this will not be used in this code\n",
    "            tokens = word_tokenize(text)\n",
    "            lemmatizer = WordNetLemmatizer() \n",
    "            tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUZSOLV3qIi0"
   },
   "source": [
    "# Semantic Text Similarity Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hULuyIkZr4G_"
   },
   "source": [
    "Pip-installing semantic text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XZ04TRSgQva",
    "outputId": "1758eb92-de4c-47f7-9cbf-d15e50bd6487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-text-similarity in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from semantic-text-similarity) (1.8.1+cu101)\n",
      "Requirement already satisfied: fuzzywuzzy[speedup] in /usr/local/lib/python3.7/dist-packages (from semantic-text-similarity) (0.18.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from semantic-text-similarity) (1.4.1)\n",
      "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.7/dist-packages (from semantic-text-similarity) (1.1.0)\n",
      "Requirement already satisfied: strsim in /usr/local/lib/python3.7/dist-packages (from semantic-text-similarity) (0.0.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->semantic-text-similarity) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->semantic-text-similarity) (1.19.5)\n",
      "Requirement already satisfied: python-levenshtein>=0.12; extra == \"speedup\" in /usr/local/lib/python3.7/dist-packages (from fuzzywuzzy[speedup]->semantic-text-similarity) (0.12.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (2019.12.20)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (0.1.95)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (1.17.84)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (4.41.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-levenshtein>=0.12; extra == \"speedup\"->fuzzywuzzy[speedup]->semantic-text-similarity) (56.1.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity) (0.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.84 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity) (1.20.84)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.84->boto3->pytorch-transformers==1.1.0->semantic-text-similarity) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.84->boto3->pytorch-transformers==1.1.0->semantic-text-similarity) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install semantic-text-similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11sUG89oqOK2"
   },
   "source": [
    "Importing Web BERT Similarity and Clinical BERT Similarity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "kWKyTewWsjrF"
   },
   "outputs": [],
   "source": [
    "from semantic_text_similarity.models import WebBertSimilarity          #Directly importing Web Bert Similarity model\n",
    "from semantic_text_similarity.models import ClinicalBertSimilarity     #Directly importing Clinical Bert Similarity model\n",
    "\n",
    "web_model = WebBertSimilarity(device='cpu', batch_size=10)             #defaults to GPU prediction\n",
    "clinical_model = ClinicalBertSimilarity(device='cuda', batch_size=10)  #defaults to GPU prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfidbVOoqXG1"
   },
   "source": [
    "# Implementing the Web BERT Similarity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYrg7lGogP6d",
    "outputId": "02af993e-dde0-4156-fbbb-36a409fd399c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#WEB BERT SIMILARITY MODEL\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "similarity = []      #Defining an empty list to store the final similarity values. \n",
    "\n",
    "for indices in text_data.index:\n",
    "    \n",
    "        s1 = text_data['text1'][indices]\n",
    "        s2 = text_data['text2'][indices]\n",
    "        s1words = word_tokenizer(s1)\n",
    "        s2words = word_tokenizer(s2)\n",
    "\n",
    "        if s1==s2:\n",
    "           similarity.append(1.0)        # 1 means highly similar\n",
    "\n",
    "        if len(s1 or s2)==0:            #If length of sentences in either of the datasets is zero, then ignore them. But we already dropped all null rows, so no need to worry.\n",
    "            similarity.append(0.0)       # 0 means highly dissimilar\n",
    "\n",
    "        else:   \n",
    "            similarity.append(web_model.predict([(s1, s2)])) #1 corresponds to highly similar sentences and 0 corresponds to highly dissimilar sentences\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "gAEQ6UTwzn6Z",
    "outputId": "af05a596-9939-4f0b-b521-142d19efd118"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.24079262]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.78855926]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.5916782]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.99573034]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.90157884]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID Similarity_score\n",
       "0          0     [0.24079262]\n",
       "1          1     [0.78855926]\n",
       "2          2      [0.5916782]\n",
       "3          3     [0.99573034]\n",
       "4          4     [0.90157884]"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Unique_ID and similarity\n",
    "similarity.pop()              # List 'similarity' has 4024 values. One extra value added at the end due to the for loop\n",
    "final_score1 = pd.DataFrame({'Unique_ID':text_data.Unique_ID, 'Similarity_score':similarity})   #Creating a new dataframe with Unique ID and Similarity score\n",
    "final_score1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "VF4ZGB6HuLrZ"
   },
   "outputs": [],
   "source": [
    "# Saving dataframe as CSV file \n",
    "final_score1.to_csv('Finalscore_WebBert.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0Su3l9wqaZR"
   },
   "source": [
    "# Implementing the Clinical BERT Similarity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyIl5b-w0FlE",
    "outputId": "335fb56a-34f4-4349-e05f-11d5dee559c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#CLINICAL BERT SIMILARITY MODEL\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "similarity = []\n",
    "for ind in text_data.index:\n",
    "    \n",
    "        s1 = text_data['text1'][ind]\n",
    "        s2 = text_data['text2'][ind]\n",
    "        \n",
    "        if s1==s2:\n",
    "           similarity.append(1.0) # 1 means highly similar\n",
    "\n",
    "        if len(s1words and s2words)==0:\n",
    "            similarity.append(0.0) \n",
    "\n",
    "        else:   \n",
    "\n",
    "            s1words = word_tokenizer(s1)\n",
    "            s2words = word_tokenizer(s2)\n",
    "            similarity.append(clinical_model.predict([(s1, s2)])) # as it is given 1 means highly similar & 0 means highly dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "sBMh-prn1znZ",
    "outputId": "81c6526a-3ba9-4816-fc1a-7d49db55a12b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.6166725]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.8093727]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1.2970107]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[1.4599866]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.762307]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID Similarity_score\n",
       "0          0      [0.6166725]\n",
       "1          1      [0.8093727]\n",
       "2          2      [1.2970107]\n",
       "3          3      [1.4599866]\n",
       "4          4       [0.762307]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Unique_ID and similarity for Clinical Bert Similarity Model\n",
    "similarity.pop()              # List 'similarity' has 4024 values. One extra value added at the end due to the for loop\n",
    "final_score2 = pd.DataFrame({'Unique_ID':text_data.Unique_ID, 'Similarity_score':similarity})       #Creating a new dataframe with Unique ID and Similarity score\n",
    "final_score2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "f7rtDC5w1zpq"
   },
   "outputs": [],
   "source": [
    "# Saving dataframe as CSV file \n",
    "final_score2.to_csv('Finalscore_ClinicalBert.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFLVTRCXqy6Q"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD3BD-tOq1c8"
   },
   "source": [
    "Here I have implemented the Web BERT Similarity Model and Clinical BERT Similarity Model. Both give very different results. More explained in report. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Semantic Text Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
